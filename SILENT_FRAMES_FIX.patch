--- Silent Frames Fix Patch ---
=====================================

This patch fixes the issue where TTS continues sending silent frames after narration ends,
instead of properly terminating the stream and draining the frame queue.

ISSUE SUMMARY:
- TTS completes but AudioStreamGenerator keeps generating silence indefinitely
- Inference API continues generating silent frames after end_of_stream marker
- Frame draining takes too long with inefficient timeouts
- End stream detection not properly handled throughout the pipeline

SOLUTION OVERVIEW:
1. Fix AudioStreamGenerator to stop after end_of_stream marker
2. Update Inference API to respect end_of_stream in buffer management
3. Enhanced stream termination logic with proper frame draining
4. Improved frame draining timeouts in Rapido system
5. Confirmed LiveKit start_stream messaging is already implemented

FILES MODIFIED:
===============

1. SyncTalk_2D/audio_stream_generator.py
2. SyncTalk_2D/inference_system/api.py
3. rapido_system/api/rapido_main.py (Note: This was in old structure, equivalent fixes needed in current rapido/src/rapido_main.py)

DETAILED CHANGES:
================

--- a/SyncTalk_2D/audio_stream_generator.py
+++ b/SyncTalk_2D/audio_stream_generator.py
@@ -54,6 +54,10 @@ class AudioStreamGenerator:
         self.last_silence_time = 0
         self.silence_chunk_duration = 0.15  # 150ms chunks for better responsiveness
         self.silence_backlog = 0.0  # Track how much silence we owe
+        
+        # NEW: Track stream end state to stop silence generation
+        self.stream_ended = False  # Track if end_of_stream received
+        self.end_marker_processed = False  # Track if we've processed the end marker
         
     async def add_audio_chunk_async(self, audio_chunk: bytes):
         """Add new audio chunk from websocket (with async resampling if needed)"""
@@ -62,6 +66,10 @@ class AudioStreamGenerator:
                 # Handle special stream markers
                 if audio_chunk in ALL_MARKERS:
                     self.audio_queue.put(audio_chunk)
+                    # NEW: Mark stream as ended when end_of_stream received
+                    if audio_chunk in END_MARKERS:
+                        self.stream_ended = True
+                        logger.info("ðŸ”š End of stream marker received - stopping silence generation")
                     return
                 
                 # Resample audio asynchronously if necessary
@@ -79,6 +87,10 @@ class AudioStreamGenerator:
                 # Handle special stream markers
                 if audio_chunk in ALL_MARKERS:
                     self.audio_queue.put(audio_chunk)
+                    # NEW: Mark stream as ended when end_of_stream received
+                    if audio_chunk in END_MARKERS:
+                        self.stream_ended = True
+                        logger.info("ðŸ”š End of stream marker received - stopping silence generation")
                     return
                 
                 # Resample audio if necessary (blocking version)
@@ -168,6 +180,18 @@ class AudioStreamGenerator:
             if chunk is None:  # Sentinel value
                 raise StopIteration
             
+            # NEW: Handle end marker and stop iteration
+            if isinstance(chunk, bytes) and chunk in END_MARKERS:
+                if not self.end_marker_processed:
+                    self.end_marker_processed = True
+                    logger.info("ðŸ”š Processing end_of_stream marker - will stop iteration after this")
+                    return chunk  # Return the marker once
+                else:
+                    logger.info("ðŸ”š End marker already processed - stopping iteration")
+                    raise StopIteration
+            
             logger.info(f"AudioStreamGenerator: Got audio chunk of {len(chunk)} bytes")
             # Reset silence tracking when we get real audio
             self.silence_backlog = 0.0
@@ -178,6 +202,11 @@ class AudioStreamGenerator:
                 if not self.is_active:
                     raise StopIteration
             
+                # NEW: Don't generate silence if stream has ended
+                if self.stream_ended:
+                    logger.info("ðŸ”š Stream ended - stopping iteration (no more silence generation)")
+                    raise StopIteration
+            
             # Log "returning None" only every 100 times
             audio_none_log_counter += 1
             # if audio_none_log_counter % 100 == 0:

--- a/SyncTalk_2D/inference_system/api.py
+++ b/SyncTalk_2D/inference_system/api.py
@@ -625,6 +625,8 @@ class InferenceAPI:
                     else:  # END_MARKERS
                         boundary_frame = streaming_dataset.get_max_encodable_frame() + 1
+                        # NEW: Mark that we should stop generating frames after draining
+                        state['end_of_stream_received'] = True
+                        print(f"ðŸ”š End of stream marker received - will stop after draining existing frames")
                     
                     marker_queue.append((audio_chunk, boundary_frame))
                     print(f"Queued marker {audio_chunk.decode()} at frame boundary {boundary_frame}")
@@ -768,6 +770,11 @@ class InferenceAPI:
         # Calculate frames still needed to maintain rate
         frames_still_needed = frames_to_generate - len(frames_to_submit)
         
+        # NEW: Don't generate silence if end of stream received
+        if state.get('end_of_stream_received', False):
+            print("ðŸ”š End of stream received - not generating more silent frames")
+            return 0
+        
         # Handle temporary silence mode
         if frames_still_needed > 0 and state['in_temporary_silence']:
             # Get buffer configuration for silence mode
@@ -1104,6 +1111,10 @@ class InferenceAPI:
                     should_generate_silence = (
                         state['in_temporary_silence'] and
                         yield_buffer_size < CRITICAL_YIELD_BUFFER_FRAMES
                     )
                     
+                    # NEW: Don't generate silence if end of stream received
+                    if state.get('end_of_stream_received', False):
+                        print("ðŸ”š End of stream received - not generating silence for yield buffer")
+                    elif should_generate_silence:
                         # Calculate total frames in flight
                         pending_in_model = len(submitted_frames) - state['stream_info']['frames_processed']
                         total_frames_in_flight = yield_buffer_size + pending_in_model
@@ -1249,6 +1260,19 @@ class InferenceAPI:
             state['last_stats_time'] = current_time
         
-        # STEP 6: Check exit conditions
+        # STEP 6: Enhanced exit conditions
         if state['audio_exhausted'] and not state['continue_silent']:
             # Process any remaining markers
             while state['marker_queue']:
                 marker, _ = state['marker_queue'].popleft()
                 frames_to_add.append((marker, None, None, marker))
             
-            # Check if all frames are processed
-            max_frame = streaming_dataset.get_max_encodable_frame()
-            all_frames_submitted = all(f in submitted_frames for f in range(max_frame + 1))
-            
-            if all_frames_submitted and state['stream_info']['frames_processed'] >= state['frames_processed']:
-                print(f"\nAll frames completed!")
-                continue_loop = False
+            # NEW: If end_of_stream was received, stop immediately after processing existing frames
+            if state.get('end_of_stream_received', False):
+                max_frame = streaming_dataset.get_max_encodable_frame()
+                frames_in_pipeline = len(submitted_frames) - state['stream_info']['frames_processed']
+                
+                print(f"ðŸ”š End of stream - draining {frames_in_pipeline} remaining frames (max_frame: {max_frame})")
+                
+                # Only continue if there are frames still being processed
+                if frames_in_pipeline <= 0:
+                    print("ðŸ”š All frames drained - stopping stream")
+                    continue_loop = False
+            else:
+                # Original logic for streams without explicit end marker
+                max_frame = streaming_dataset.get_max_encodable_frame()
+                all_frames_submitted = all(f in submitted_frames for f in range(max_frame + 1))
+                
+                if all_frames_submitted and state['stream_info']['frames_processed'] >= state['frames_processed']:
+                    print(f"\nAll frames completed!")
+                    continue_loop = False

--- a/rapido_system/api/rapido_main.py (OLD STRUCTURE - EQUIVALENT CHANGES NEEDED IN CURRENT rapido/src/rapido_main.py)
+++ b/rapido_system/api/rapido_main.py
@@ -983,11 +983,11 @@ class RapidoMainSystem:
 
         # Drain remaining frames until:
         # 1) No new frames arrive for a short quiet period AND
         # 2) The intake queue is empty for a short continuous period.
-        # No timeout: fully flush all frames after audio ends.
-        quiet_required_seconds = 1.0
-        empty_required_seconds = 1.0
+        # ENHANCED: More responsive frame draining with shorter timeouts
+        quiet_required_seconds = 0.5  # Reduced from 1.0 for faster response
+        empty_required_seconds = 0.5  # Reduced from 1.0 for faster response
+        check_interval = 0.2  # Reduced from 0.5 for more responsive checking
+        max_drain_time = 10.0  # Maximum time to wait for draining
+        
         last_received_count = self.frames_received
         quiet_elapsed = 0.0
         empty_elapsed = 0.0
-        logger.info("â³ Draining remaining frames until queue empty and quiet period (no timeout)...")
+        drain_start_time = time.time()
+        
+        logger.info("â³ Enhanced frame draining - shorter timeouts, more responsive...")
+        
         while True:
-            await asyncio.sleep(0.5)
+            await asyncio.sleep(check_interval)
+            
+            # Check for overall timeout
+            if time.time() - drain_start_time > max_drain_time:
+                logger.warning(f"âš ï¸ Frame draining timeout after {max_drain_time}s - forcing completion")
+                break
+            
             # Check quiet period on incoming frames
             current_received = self.frames_received
             if current_received == last_received_count:
-                quiet_elapsed += 0.5
+                quiet_elapsed += check_interval
             else:
                 quiet_elapsed = 0.0
                 last_received_count = current_received
+            
             # Check output queue emptiness
             qsize = self.intake_queue.qsize()
             if qsize == 0:
-                empty_elapsed += 0.5
+                empty_elapsed += check_interval
             else:
                 empty_elapsed = 0.0
-            # Periodic status
-            if int(time.time()) % 5 == 0:
+            
+            # More frequent status updates for better visibility
+            if int(time.time() * 2) % 5 == 0:  # Every 2.5 seconds
                 logger.info(f"ðŸ§º Drain status: queue={qsize}, quiet={quiet_elapsed:.1f}s, empty={empty_elapsed:.1f}s, received={current_received}")
+            
             # Exit when both conditions satisfied
             if quiet_elapsed >= quiet_required_seconds and empty_elapsed >= empty_required_seconds:
                 logger.info(
-                    f"âœ… Drain complete: queue empty ({empty_elapsed:.1f}s) and quiet ({quiet_elapsed:.1f}s). "
+                    f"âœ… Enhanced drain complete: queue empty ({empty_elapsed:.1f}s) and quiet ({quiet_elapsed:.1f}s). "
                     f"Total frames received: {current_received}"
                 )
                 break

LIVEKIT START_STREAM STATUS:
===========================

âœ… ALREADY IMPLEMENTED IN CURRENT BRANCH!

The current rapido/src/rapido_main.py already has complete LiveKit data messaging:

1. âœ… send_stream_event_to_frontend() method (line 1284)
2. âœ… Uses proper LiveKit publish_data() on "control" topic (line 1304)
3. âœ… Sends "stream_started" event when buffer fills (line 989)
4. âœ… Sends "stream_ended" event on completion (rapido_api.py line 429)
5. âœ… Sends "stream_stopped" event on manual stop (line 1250)

Event format:
```json
{
  "id": "avatar_event_{timestamp}",
  "event": "stream_started|stream_ended|stream_stopped",
  "timestamp": 1234567890,
  "message": "Human readable message",
  ...additional_data
}
```

TESTING INSTRUCTIONS:
====================

1. Apply the SyncTalk fixes to audio_stream_generator.py and inference_system/api.py
2. The equivalent rapido_main.py fixes should be applied to current rapido/src/rapido_main.py 
3. Test that:
   - TTS completes and immediately sends end_of_stream marker
   - Silent frame generation stops immediately after end marker
   - Frame queue drains within 0.5-1.0 seconds (not indefinitely)
   - LiveKit frontend receives "stream_started" and "stream_ended" events
   - Final MP4 is created without excessive silent frames

EXPECTED BEHAVIOR AFTER PATCH:
==============================

1. âœ… TTS completes â†’ end_of_stream sent immediately
2. âœ… AudioStreamGenerator stops generating silence
3. âœ… Inference API stops generating silent frames  
4. âœ… Frame queue drains quickly (0.5s timeout)
5. âœ… Frontend receives proper start/end events via LiveKit data channel
6. âœ… Final video created with correct length (no extra silent frames)

ROOT CAUSE FIXED:
=================

The system was treating stream completion the same as temporary silence, 
causing infinite silent frame generation. Now it properly distinguishes between:
- Temporary silence (during speech pauses) â†’ generate silent frames
- Stream completion (after end_of_stream) â†’ stop generating frames and drain

This patch ensures clean stream termination with immediate response to TTS completion.
