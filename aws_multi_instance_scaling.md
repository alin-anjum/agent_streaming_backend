# AWS Multi-Instance Scaling Architecture for AI Avatar Backend

## Current Problem Analysis

**Single Instance Limitations:**
- Max concurrent streams: 5 (Tesla T4 limit)
- Single point of failure  
- Cannot scale beyond physical GPU memory
- Network bandwidth bottleneck on single instance
- No geographic distribution possible

## True Multi-Instance Architecture

### 1. Hub & Spoke Architecture Overview

```mermaid
graph TB
    subgraph "Internet"
        USERS[Users Worldwide]
    end
    
    subgraph "AWS Route 53 + CloudFront"
        DNS[rapido.yourdomain.com]
        CDN[CloudFront Distribution]
    end
    
    subgraph "Hub Layer (Control Plane)"
        ALB[Application Load Balancer]
        subgraph "API Gateway Cluster (t3.medium)"
            HUB1[Hub Instance 1]
            HUB2[Hub Instance 2] 
            HUB3[Hub Instance 3]
        end
        REDIS[ElastiCache Redis Cluster]
        RDS[RDS PostgreSQL]
    end
    
    subgraph "Worker Layer (Data Plane)"
        subgraph "US-East-1 GPU Cluster"
            GPU1[g5.xlarge - GPU Worker 1<br/>RTX A10G - 12 streams]
            GPU2[g5.xlarge - GPU Worker 2<br/>RTX A10G - 12 streams]
            GPU3[g5.2xlarge - GPU Worker 3<br/>RTX A10G - 24 streams]
        end
        
        subgraph "US-West-2 GPU Cluster"
            GPU4[g5.xlarge - GPU Worker 4<br/>RTX A10G - 12 streams]
            GPU5[g5.xlarge - GPU Worker 5<br/>RTX A10G - 12 streams]
            GPU6[g5.2xlarge - GPU Worker 6<br/>RTX A10G - 24 streams]
        end
        
        subgraph "EU-Central-1 GPU Cluster"
            GPU7[g5.xlarge - GPU Worker 7<br/>RTX A10G - 12 streams]
            GPU8[g5.xlarge - GPU Worker 8<br/>RTX A10G - 12 streams]
        end
    end
    
    subgraph "LiveKit Multi-Region"
        LK_US_EAST[LiveKit US-East]
        LK_US_WEST[LiveKit US-West]
        LK_EU[LiveKit EU]
    end
    
    USERS --> DNS
    DNS --> CDN
    CDN --> ALB
    ALB --> HUB1
    ALB --> HUB2
    ALB --> HUB3
    
    HUB1 <--> REDIS
    HUB2 <--> REDIS
    HUB3 <--> REDIS
    HUB1 <--> RDS
    
    HUB1 --> GPU1
    HUB1 --> GPU2
    HUB2 --> GPU3
    HUB2 --> GPU4
    HUB3 --> GPU5
    HUB3 --> GPU6
    
    GPU1 --> LK_US_EAST
    GPU2 --> LK_US_EAST
    GPU4 --> LK_US_WEST
    GPU5 --> LK_US_WEST
    GPU7 --> LK_EU
```

### 2. AWS Infrastructure Components

#### A. Hub Layer (Control Plane)
```yaml
# Hub instances handle routing and orchestration
Hub_Instances:
  Instance_Type: t3.medium (2 vCPU, 4GB RAM)
  Auto_Scaling_Group:
    Min: 2
    Max: 10
    Target: 3
  Role: 
    - Request routing
    - Load balancing
    - Session management
    - Health monitoring
    - LiveKit region selection
  
Load_Balancer:
  Type: Application Load Balancer
  SSL_Termination: Yes
  Health_Check: /health endpoint
  Sticky_Sessions: Yes (for session affinity)
```

#### B. Worker Layer (GPU Processing)
```yaml
# GPU worker instances for SyncTalk processing
GPU_Workers:
  Instance_Types:
    - g5.xlarge:   # 4 vCPU, 16GB RAM, 1x RTX A10G (24GB VRAM)
        Concurrent_Streams: 12
        Cost: ~$1.20/hour
    - g5.2xlarge:  # 8 vCPU, 32GB RAM, 1x RTX A10G (24GB VRAM)  
        Concurrent_Streams: 24
        Cost: ~$2.40/hour
    - g4dn.2xlarge: # 8 vCPU, 32GB RAM, 1x Tesla T4 (16GB VRAM)
        Concurrent_Streams: 8
        Cost: ~$0.75/hour (fallback option)
  
  Auto_Scaling_Groups:
    US_East_1:
      Min: 2
      Max: 20
      Target: 4
    US_West_2:
      Min: 1
      Max: 15
      Target: 3
    EU_Central_1:
      Min: 1
      Max: 10
      Target: 2
```

#### C. Supporting Services
```yaml
ElastiCache_Redis:
  Purpose: Session routing, worker discovery, load metrics
  Instance: cache.r6g.large
  Nodes: 3 (Multi-AZ)
  
RDS_PostgreSQL:
  Purpose: Persistent session data, analytics, user management
  Instance: db.t3.medium
  Multi_AZ: Yes
  
CloudFront:
  Purpose: Static content, API caching, global edge locations
  Behaviors:
    - /api/*: Pass to ALB
    - /static/*: Cache at edge
    - /ws/*: WebSocket passthrough
```

### 3. Hub Implementation (Smart Router)

```python
# aws-hub/smart_router.py
import boto3
import redis
import asyncio
from typing import List, Dict, Optional
from geopy.distance import geodesic

class SmartHubRouter:
    def __init__(self):
        self.ec2 = boto3.client('ec2')
        self.redis_cluster = redis.RedisCluster(
            startup_nodes=[
                {"host": "rapido-redis.cache.amazonaws.com", "port": "6379"}
            ]
        )
        self.worker_discovery = WorkerDiscoveryService()
        
    async def route_session_request(self, lesson_id: str, user_location: dict) -> Dict:
        """
        Intelligent routing logic:
        1. Check for existing session affinity
        2. Find optimal GPU worker based on location + load
        3. Select best LiveKit region
        4. Reserve resources and return connection info
        """
        
        # Check existing session
        existing_session = await self.redis_cluster.get(f"session:{lesson_id}")
        if existing_session:
            session_data = json.loads(existing_session)
            if await self.validate_worker_health(session_data['worker_id']):
                return session_data
            else:
                # Worker is down, need to reassign
                await self.cleanup_stale_session(lesson_id)
        
        # Find optimal worker
        optimal_worker = await self.find_optimal_worker(user_location)
        if not optimal_worker:
            raise NoAvailableWorkersException("All GPU workers at capacity")
        
        # Reserve capacity
        reserved = await self.reserve_worker_capacity(optimal_worker['worker_id'])
        if not reserved:
            # Race condition - try next best worker
            return await self.route_session_request(lesson_id, user_location)
        
        # Select LiveKit region
        livekit_config = await self.select_livekit_region(
            user_location, 
            optimal_worker['region']
        )
        
        # Create session record
        session_config = {
            'lesson_id': lesson_id,
            'worker_id': optimal_worker['worker_id'],
            'worker_url': optimal_worker['internal_url'],
            'livekit_url': livekit_config['url'],
            'livekit_region': livekit_config['region'],
            'created_at': time.time(),
            'user_location': user_location
        }
        
        # Store session with TTL
        await self.redis_cluster.setex(
            f"session:{lesson_id}",
            3600,  # 1 hour TTL
            json.dumps(session_config)
        )
        
        return session_config
        
    async def find_optimal_worker(self, user_location: dict) -> Optional[Dict]:
        """Find best GPU worker based on location and current load"""
        
        # Get all healthy workers
        workers = await self.worker_discovery.get_healthy_workers()
        
        scored_workers = []
        for worker in workers:
            # Calculate score based on:
            # 1. Geographic distance (40% weight)
            # 2. Current load (30% weight)  
            # 3. Historical performance (20% weight)
            # 4. Available capacity (10% weight)
            
            distance_score = self.calculate_distance_score(
                user_location, worker['location']
            )
            load_score = worker['current_load'] / worker['max_capacity']
            performance_score = worker['avg_response_time_ms'] / 1000
            capacity_score = worker['available_streams'] / worker['max_streams']
            
            total_score = (
                distance_score * 0.4 +
                (1 - load_score) * 0.3 +
                (1 - performance_score) * 0.2 +
                capacity_score * 0.1
            )
            
            scored_workers.append((total_score, worker))
        
        # Sort by score (higher is better)
        scored_workers.sort(key=lambda x: x[0], reverse=True)
        
        # Return best worker if it has available capacity
        for score, worker in scored_workers:
            if worker['available_streams'] > 0:
                return worker
                
        return None

class WorkerDiscoveryService:
    """Service for discovering and monitoring GPU workers"""
    
    def __init__(self):
        self.ec2 = boto3.client('ec2')
        self.redis_cluster = redis.RedisCluster(
            startup_nodes=[
                {"host": "rapido-redis.cache.amazonaws.com", "port": "6379"}
            ]
        )
    
    async def register_worker(self, worker_info: Dict):
        """GPU workers call this to register themselves"""
        worker_id = worker_info['worker_id']
        
        # Store worker info in Redis with TTL
        await self.redis_cluster.setex(
            f"worker:{worker_id}",
            300,  # 5 minute TTL - workers must heartbeat
            json.dumps({
                **worker_info,
                'last_heartbeat': time.time(),
                'status': 'healthy'
            })
        )
        
        # Add to active workers set
        await self.redis_cluster.sadd("active_workers", worker_id)
    
    async def get_healthy_workers(self) -> List[Dict]:
        """Get all currently healthy GPU workers"""
        active_workers = await self.redis_cluster.smembers("active_workers")
        healthy_workers = []
        
        for worker_id in active_workers:
            worker_data = await self.redis_cluster.get(f"worker:{worker_id}")
            if worker_data:
                worker_info = json.loads(worker_data)
                
                # Check if heartbeat is recent (< 5 minutes)
                if time.time() - worker_info['last_heartbeat'] < 300:
                    healthy_workers.append(worker_info)
                else:
                    # Remove stale worker
                    await self.redis_cluster.srem("active_workers", worker_id)
                    await self.redis_cluster.delete(f"worker:{worker_id}")
        
        return healthy_workers
```

### 4. GPU Worker Implementation

```python
# gpu-worker/worker_node.py
import asyncio
import aiohttp
import json
import time
from typing import Dict
import boto3
import psutil
import GPUtil

class GPUWorkerNode:
    def __init__(self, worker_id: str, hub_urls: List[str]):
        self.worker_id = worker_id
        self.hub_urls = hub_urls
        self.max_streams = self.detect_gpu_capacity()
        self.current_streams = 0
        self.synctalk_server = None
        self.health_metrics = HealthMetrics()
        
    async def start(self):
        """Start the GPU worker node"""
        # Start SyncTalk server
        await self.start_synctalk_server()
        
        # Register with hub
        await self.register_with_hub()
        
        # Start heartbeat loop
        asyncio.create_task(self.heartbeat_loop())
        
        # Start metrics collection
        asyncio.create_task(self.metrics_collection_loop())
        
        # Start FastAPI server for receiving requests
        await self.start_api_server()
    
    def detect_gpu_capacity(self) -> int:
        """Detect GPU capacity and determine max concurrent streams"""
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]  # Assume single GPU per instance
                
                # Capacity mapping based on GPU memory
                if gpu.memoryTotal >= 20000:  # RTX A10G (24GB)
                    return 12 if 'g5.xlarge' in self.get_instance_type() else 24
                elif gpu.memoryTotal >= 15000:  # Tesla T4 (16GB)
                    return 8
                else:
                    return 4
            else:
                return 2  # Fallback for CPU-only
        except Exception:
            return 2  # Safe fallback
    
    async def register_with_hub(self):
        """Register this worker with the hub cluster"""
        instance_metadata = self.get_instance_metadata()
        
        worker_info = {
            'worker_id': self.worker_id,
            'instance_id': instance_metadata['instance_id'],
            'instance_type': instance_metadata['instance_type'],
            'availability_zone': instance_metadata['availability_zone'],
            'region': instance_metadata['region'],
            'internal_url': f"http://{instance_metadata['private_ip']}:8000",
            'public_url': f"http://{instance_metadata['public_ip']}:8000",
            'max_streams': self.max_streams,
            'current_streams': self.current_streams,
            'available_streams': self.max_streams - self.current_streams,
            'gpu_info': self.get_gpu_info(),
            'location': self.get_geographic_location(),
            'avg_response_time_ms': self.health_metrics.get_avg_response_time(),
            'current_load': self.current_streams / self.max_streams
        }
        
        # Try registering with each hub until successful
        for hub_url in self.hub_urls:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{hub_url}/api/v1/worker/register",
                        json=worker_info
                    ) as response:
                        if response.status == 200:
                            print(f"‚úÖ Registered with hub: {hub_url}")
                            return
            except Exception as e:
                print(f"‚ùå Failed to register with hub {hub_url}: {e}")
        
        raise Exception("Failed to register with any hub")
    
    async def heartbeat_loop(self):
        """Send periodic heartbeat to maintain registration"""
        while True:
            try:
                await self.send_heartbeat()
                await asyncio.sleep(60)  # Heartbeat every minute
            except Exception as e:
                print(f"Heartbeat error: {e}")
                await asyncio.sleep(10)  # Retry quickly on error
    
    async def send_heartbeat(self):
        """Send current status to hub"""
        status_info = {
            'worker_id': self.worker_id,
            'current_streams': self.current_streams,
            'available_streams': self.max_streams - self.current_streams,
            'cpu_percent': psutil.cpu_percent(),
            'memory_percent': psutil.virtual_memory().percent,
            'gpu_utilization': self.get_gpu_utilization(),
            'avg_response_time_ms': self.health_metrics.get_avg_response_time(),
            'error_rate': self.health_metrics.get_error_rate(),
            'timestamp': time.time()
        }
        
        for hub_url in self.hub_urls:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{hub_url}/api/v1/worker/heartbeat",
                        json=status_info
                    ) as response:
                        if response.status == 200:
                            break  # Successfully sent to at least one hub
            except Exception:
                continue  # Try next hub
```

### 5. Auto-Scaling Configuration

```yaml
# cloudformation-templates/gpu-worker-autoscaling.yaml
AWSTemplateFormatVersion: '2010-09-09'

Parameters:
  VpcId:
    Type: AWS::EC2::VPC::Id
  SubnetIds:
    Type: List<AWS::EC2::Subnet::Id>
  KeyPair:
    Type: AWS::EC2::KeyPair::KeyName

Resources:
  # Launch Template for GPU Workers
  GPUWorkerLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateName: rapido-gpu-worker
      LaunchTemplateData:
        ImageId: ami-0c94855ba95b798c7  # Deep Learning AMI (Ubuntu)
        InstanceType: g5.xlarge
        KeyName: !Ref KeyPair
        IamInstanceProfile:
          Name: !Ref GPUWorkerInstanceProfile
        SecurityGroupIds:
          - !Ref GPUWorkerSecurityGroup
        UserData:
          Fn::Base64: !Sub |
            #!/bin/bash
            yum update -y
            
            # Install Docker
            amazon-linux-extras install docker -y
            systemctl start docker
            systemctl enable docker
            usermod -a -G docker ec2-user
            
            # Install NVIDIA Docker
            curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | apt-key add -
            curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu20.04/nvidia-docker.list | tee /etc/apt/sources.list.d/nvidia-docker.list
            apt-get update && apt-get install -y nvidia-docker2
            systemctl restart docker
            
            # Download and start GPU worker
            aws s3 cp s3://rapido-deployment/gpu-worker.tar.gz /tmp/
            cd /tmp && tar -xzf gpu-worker.tar.gz
            cd gpu-worker && ./start-worker.sh
        TagSpecifications:
          - ResourceType: instance
            Tags:
              - Key: Name
                Value: rapido-gpu-worker
              - Key: Application
                Value: rapido-avatar-backend
              - Key: Role
                Value: gpu-worker

  # Auto Scaling Group
  GPUWorkerAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      AutoScalingGroupName: rapido-gpu-workers
      LaunchTemplate:
        LaunchTemplateId: !Ref GPUWorkerLaunchTemplate
        Version: !GetAtt GPUWorkerLaunchTemplate.LatestVersionNumber
      MinSize: 2
      MaxSize: 20
      DesiredCapacity: 4
      VPCZoneIdentifier: !Ref SubnetIds
      HealthCheckType: ELB
      HealthCheckGracePeriod: 300
      Tags:
        - Key: Name
          Value: rapido-gpu-worker
          PropagateAtLaunch: true

  # Scaling Policies
  ScaleUpPolicy:
    Type: AWS::AutoScaling::ScalingPolicy
    Properties:
      AutoScalingGroupName: !Ref GPUWorkerAutoScalingGroup
      PolicyType: TargetTrackingScaling
      TargetTrackingConfiguration:
        PredefinedMetricSpecification:
          PredefinedMetricType: ASGAverageCPUUtilization
        TargetValue: 70.0
        ScaleOutCooldown: 300
        ScaleInCooldown: 300

  # Custom CloudWatch Metrics for GPU Utilization
  GPUUtilizationAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: GPU-High-Utilization
      AlarmDescription: Trigger scaling when GPU utilization is high
      MetricName: GPUUtilization
      Namespace: Rapido/GPU
      Statistic: Average
      Period: 300
      EvaluationPeriods: 2
      Threshold: 80
      ComparisonOperator: GreaterThanThreshold
      AlarmActions:
        - !Ref ScaleUpPolicy
```

### 6. Cost Optimization & Scaling Strategy

```python
# cost-optimizer/instance_manager.py
class CostOptimizedScaling:
    """
    Intelligent scaling that minimizes costs while maintaining performance
    """
    
    def __init__(self):
        self.ec2 = boto3.client('ec2')
        self.pricing = boto3.client('pricing', region_name='us-east-1')
        
    async def optimize_instance_mix(self, demand_forecast: Dict) -> Dict:
        """
        Optimize instance types and count based on predicted demand
        """
        
        # Instance options with capabilities
        instance_options = [
            {
                'type': 'g5.xlarge',
                'streams': 12,
                'cost_per_hour': 1.20,
                'startup_time_seconds': 90
            },
            {
                'type': 'g4dn.2xlarge', 
                'streams': 8,
                'cost_per_hour': 0.75,
                'startup_time_seconds': 120
            },
            {
                'type': 'g5.2xlarge',
                'streams': 24,
                'cost_per_hour': 2.40,
                'startup_time_seconds': 90
            }
        ]
        
        # Spot instance strategy for cost savings
        spot_savings = await self.get_spot_pricing()
        
        optimal_mix = self.calculate_optimal_instance_mix(
            demand_forecast,
            instance_options,
            spot_savings
        )
        
        return optimal_mix
    
    def calculate_total_capacity(self) -> Dict:
        """Calculate theoretical maximum capacity"""
        
        # With multi-instance architecture:
        capacity_analysis = {
            'current_single_instance': {
                'max_streams': 5,
                'cost_per_month': 300,
                'cost_per_stream_per_month': 60
            },
            'multi_instance_scaling': {
                'instances': {
                    'g5.xlarge': {'count': 10, 'streams_each': 12, 'cost_hour': 1.20},
                    'g5.2xlarge': {'count': 5, 'streams_each': 24, 'cost_hour': 2.40}
                },
                'total_streams': (10 * 12) + (5 * 24),  # 240 streams
                'total_cost_per_hour': (10 * 1.20) + (5 * 2.40),  # $24/hour
                'total_cost_per_month': 24 * 24 * 30,  # $17,280/month
                'cost_per_stream_per_month': 17280 / 240  # $72/stream/month
            }
        }
        
        return capacity_analysis
```

### 7. Deployment Script

```bash
#!/bin/bash
# deploy-multi-instance.sh

echo "üöÄ Deploying Multi-Instance AI Avatar Backend"

# 1. Create VPC and networking
aws cloudformation create-stack \
    --stack-name rapido-networking \
    --template-body file://cloudformation/networking.yaml

# 2. Deploy Redis cluster
aws cloudformation create-stack \
    --stack-name rapido-redis \
    --template-body file://cloudformation/redis-cluster.yaml

# 3. Deploy RDS database
aws cloudformation create-stack \
    --stack-name rapido-database \
    --template-body file://cloudformation/rds.yaml

# 4. Deploy hub auto-scaling group  
aws cloudformation create-stack \
    --stack-name rapido-hub \
    --template-body file://cloudformation/hub-autoscaling.yaml

# 5. Deploy GPU worker auto-scaling groups (multi-region)
for region in us-east-1 us-west-2 eu-central-1; do
    aws cloudformation create-stack \
        --region $region \
        --stack-name rapido-gpu-workers-$region \
        --template-body file://cloudformation/gpu-worker-autoscaling.yaml
done

# 6. Deploy Application Load Balancer
aws cloudformation create-stack \
    --stack-name rapido-load-balancer \
    --template-body file://cloudformation/load-balancer.yaml

# 7. Update Route 53 to point to load balancer
aws route53 change-resource-record-sets \
    --hosted-zone-id Z1D633PJN98FT9 \
    --change-batch file://route53-changes.json

echo "‚úÖ Multi-instance deployment complete!"
echo "üìä Capacity: 5 streams ‚Üí 240+ streams (48x improvement)"
echo "üí∞ Cost per stream: $60 ‚Üí $72 (20% increase for 48x capacity)"
echo "üåç Geographic coverage: Single region ‚Üí Multi-region"
echo "üîß URL preserved: rapido.yourdomain.com"
```

## Summary: True Scalability Achieved

**Before (Single Instance):**
- ‚ùå Max 5 concurrent streams  
- ‚ùå Single point of failure
- ‚ùå No geographic distribution
- ‚ùå Fixed capacity limits

**After (Multi-Instance Hub Architecture):**
- ‚úÖ **240+ concurrent streams** (48x improvement)
- ‚úÖ **Multi-region deployment** (US East, US West, EU)  
- ‚úÖ **Auto-scaling** based on demand
- ‚úÖ **Original URL preserved** (`rapido.yourdomain.com`)
- ‚úÖ **Cost-optimized** with spot instances
- ‚úÖ **No single point of failure** 
- ‚úÖ **Intelligent routing** based on user location
- ‚úÖ **LiveKit multi-region** for optimal streaming

This architecture can scale from 5 users to 1000+ users by simply adding more GPU worker instances across regions, with the hub automatically routing requests to the optimal workers.
