# AI Avatar Streaming Backend

A comprehensive real-time AI avatar presentation system with LiveKit streaming, combining ElevenLabs TTS, SyncTalk_2D avatar generation, and dynamic frame composition.

## ğŸš€ Overview

This project creates a complete pipeline for generating and streaming AI-powered avatar videos in real-time:

- **Rapido Client**: Orchestrates the entire real-time pipeline
- **SyncTalk_2D Server**: AI-powered avatar generation using NeRF technology
- **ElevenLabs Integration**: Real-time text-to-speech streaming
- **LiveKit Streaming**: Real-time video/audio broadcasting
- **Frame Composition**: Dynamic overlay of avatar frames onto slide backgrounds
- **React Frontend**: Standalone LiveKit viewer with Creatium-style UI

## ğŸ—ï¸ System Architecture

```
JSON Input â†’ ElevenLabs TTS Stream â†’ SyncTalk Server â†’ Avatar Frames â†’ Frame Compositor â†’ LiveKit Stream â†’ React Frontend
```

## âœ¨ Features

- **Real-time streaming** with ElevenLabs TTS and SyncTalk inference
- **LiveKit integration** for low-latency video/audio streaming
- **Chroma keying** (green screen removal) with configurable parameters
- **Dynamic frame composition** with slide looping
- **Standalone React frontend** with modern UI
- **JWT-based authentication** for secure streaming
- **CUDA acceleration** for GPU processing
- **WebSocket communication** for real-time data flow
- **Comprehensive error handling** and logging

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Node.js 16+ (for frontend)
- CUDA-compatible GPU (recommended)
- LiveKit account and credentials

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/alin-anjum/agent_streaming_backend.git
cd agent_streaming_backend
```

2. **Install Python dependencies:**
```bash
pip install -r requirements.txt
cd rapido && pip install -r requirements.txt
```

3. **Set up environment variables:**
```bash
export ELEVEN_API_KEY="your_elevenlabs_api_key"
export ELEVENLABS_VOICE_ID="pNInz6obpgDQGcFmaJgB"
export LIVEKIT_URL="wss://your-livekit-url"
export LIVEKIT_API_KEY="your_api_key"
export LIVEKIT_API_SECRET="your_api_secret"
```

4. **Install frontend dependencies:**
```bash
cd avatar-frontend
npm install
```

### Running the System

#### Option 1: Automated Startup (Recommended)
```bash
cd rapido
python start_system.py
```
This will automatically start SyncTalk server, wait for it to be ready, then launch Rapido.

#### Option 2: Manual Startup

1. **Start SyncTalk server:**
```bash
cd SyncTalk_2D
python synctalk_fastapi.py
```

2. **Start Rapido client:**
```bash
cd rapido
python run_rapido.py
```

3. **Start frontend (separate terminal):**
```bash
cd avatar-frontend
node token-server.js &  # Start JWT token server
npm start              # Start React app
```

### Testing the Setup

```bash
# Test environment and dependencies
cd tests
python test_setup.py

# Run component tests
python test_rapido_components.py
```

## ğŸ“ Project Structure

```
agent_streaming_backend/
â”œâ”€â”€ rapido/                          # ğŸš€ Main AI avatar pipeline
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ rapido_main.py          # Main orchestrator with LiveKit
â”‚   â”‚   â”œâ”€â”€ tts_client.py           # ElevenLabs real-time streaming
â”‚   â”‚   â”œâ”€â”€ synctalk_client.py      # WebSocket client for SyncTalk
â”‚   â”‚   â”œâ”€â”€ frame_processor.py      # Frame composition engine
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ run_rapido.py               # Simple launcher
â”‚   â”œâ”€â”€ start_system.py             # Full system orchestrator
â”‚   â””â”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ SyncTalk_2D/                     # ğŸ¤– Avatar inference server
â”‚   â”œâ”€â”€ synctalk_fastapi.py         # FastAPI WebSocket server
â”‚   â”œâ”€â”€ avatar_config.json          # Avatar & chroma key config
â”‚   â”œâ”€â”€ chroma_key.py              # Green screen processing
â”‚   â””â”€â”€ inference_system/           # AI inference pipeline
â”œâ”€â”€ avatar-frontend/                 # ğŸ¨ Standalone React viewer
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js                  # LiveKit viewer component
â”‚   â”‚   â””â”€â”€ App.css                 # Creatium-style UI
â”‚   â”œâ”€â”€ token-server.js             # JWT token generator
â”‚   â””â”€â”€ package.json                # React dependencies
â”œâ”€â”€ tests/                          # ğŸ§ª All test files
â”‚   â”œâ”€â”€ test_setup.py               # Environment validation
â”‚   â”œâ”€â”€ test_rapido_components.py   # Component tests
â”‚   â””â”€â”€ test_*.py                   # Functional tests
â”œâ”€â”€ frames/                         # Input slide frames (PNG)
â”œâ”€â”€ test1.json                      # Sample presentation data
â””â”€â”€ README.md
```

## âš™ï¸ Configuration

### Avatar Models

Configure in `SyncTalk_2D/avatar_config.json`:

```json
{
  "enrique_torres": {
    "model_path": "model/checkpoints/enrique_torres.pth",
    "video_path": "model/Alin-cc-dataset/enrique_torres.mp4",
    "chroma_key": {
      "color_threshold": 35,
      "edge_blur": 0.4,
      "despill_factor": 0.9
    }
  }
}
```

### LiveKit Settings

```bash
LIVEKIT_URL="wss://your-project.livekit.cloud"
LIVEKIT_API_KEY="your_api_key"
LIVEKIT_API_SECRET="your_secret"
```

### Audio/Video Settings

- **Audio**: 16kHz PCM (converted from ElevenLabs MP3)
- **Video**: 1920x1080, 30 FPS
- **Streaming**: Real-time frame-by-frame publishing

## ğŸ›ï¸ API Endpoints

### SyncTalk Server (Port 8000)
- `GET /status`: Server health check
- `WebSocket /audio_to_video`: Real-time avatar inference
- Parameters: `avatar_name`, `sample_rate`

### Frontend Token Server (Port 3001)
- `GET /token`: Generate LiveKit JWT tokens
- Parameters: `room`, `identity`

### React Frontend (Port 3000)
- LiveKit room viewer with audio/video tracks
- Automatic token fetching and connection
- Modern Creatium-style interface

## ğŸ§ª Testing

Comprehensive test suite organized in `tests/` folder:

```bash
# Environment validation
python tests/test_setup.py

# Component testing
python tests/test_rapido_components.py

# End-to-end testing
python tests/test_end_to_end_synctalk.py
```

## ğŸ”§ Advanced Usage

### Custom Chroma Key Settings

Update `SyncTalk_2D/chroma_key.py` or `avatar_config.json`:
- `color_threshold`: 35 (green detection sensitivity)
- `edge_blur`: 0.4 (edge smoothing)
- `despill_factor`: 0.9 (green spill removal)

### Slide Frame Management

- Place PNG frames in `frames/` directory
- Naming: `frame_00001.png`, `frame_00002.png`, etc.
- Automatic looping if fewer slides than video duration

### LiveKit Room Management

```javascript
// Frontend automatically connects to room
const LIVEKIT_URL = 'wss://your-project.livekit.cloud';
const room = 'avatar-presentation';
```

## ğŸ› Troubleshooting

### CUDA Issues
```bash
# Check GPU compatibility
nvidia-smi

# Install compatible PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### LiveKit Connection Issues
- Verify API credentials in environment variables
- Check JWT token generation in browser dev tools
- Ensure WebSocket connection to LiveKit cloud

### Port Conflicts
```bash
# Windows
netstat -ano | findstr :8000
taskkill /F /PID <process_id>

# Linux/Mac
lsof -i :8000
kill -9 <process_id>
```

### Frontend Issues
```bash
# Clear React cache
cd avatar-frontend
rm -rf node_modules package-lock.json
npm install
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass (`python tests/test_setup.py`)
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **SyncTalk** for avatar generation technology
- **ElevenLabs** for real-time text-to-speech services
- **LiveKit** for real-time streaming infrastructure
- **FastAPI** for the WebSocket server framework
- **React** for the frontend framework
- **PyTorch** for deep learning capabilities

## ğŸ”— Links

- [ElevenLabs API Documentation](https://elevenlabs.io/docs)
- [LiveKit Documentation](https://docs.livekit.io/)
- [SyncTalk Research Paper](https://github.com/ZiqiaoPeng/SyncTalk)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

---

**Built with â¤ï¸ for real-time AI avatar streaming**